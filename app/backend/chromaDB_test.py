import os
import chromadb
from google.generativeai import GenerativeModel, configure
from app.backend.fixed_size_chunker import FixedSizeChunker

class ChromaDBResponder:
    def __init__(self, collection_name="my_collection", chunk_size=100):
        """
        Initializes the responder:
          - Creates an instance of FixedSizeChunker.
          - Initializes the Chroma client and obtains a collection.
          - Configures the Gemini generative model with a manually provided API key.
        
        Parameters:
            collection_name (str): Name of the Chroma collection.
            chunk_size (int): Fixed chunk size (number of words) for text splitting.
        """
        self.chunker = FixedSizeChunker(chunk_size)
        self.client = chromadb.Client()
        self.collection = self.client.get_or_create_collection(name=collection_name)
        
        # Configure the Gemini model with a manually provided API key.
        # Replace the key below with your actual Google Generative AI API key.
        
        configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.gemini_model = GenerativeModel("gemini-1.5-pro-latest")

    def ingest_file(self, file_path):
        """
        Reads a Markdown file, splits it into fixed-size chunks using FixedSizeChunker,
        and upserts the chunks into the Chroma collection.
        
        Parameters:
            file_path (str): Path to the Markdown file.
        """
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

        chunks = self.chunker.chunk_text(text)
        ids = [f"chunk_{i}" for i in range(len(chunks))]
        
        # Upsert the chunks into the collection (Chroma automatically embeds the text)
        self.collection.upsert(
            documents=chunks,
            ids=ids
        )
        print(f"Ingested {len(chunks)} chunks into collection '{self.collection.name}'.")

    def _count_tokens(self, text):
        """
        A simple token counter that approximates token count by splitting on whitespace.
        
        Parameters:
            text (str): The input text.
            
        Returns:
            int: The approximate number of tokens.
        """
        return len(text.split())

    def query(self, query_text, n_results=3):
        """
        Converts the user query into an embedding using Chroma's query mechanism,
        retrieves the most similar chunks, and uses them as context to generate
        a precise answer with the Gemini model.
        Additionally, prints the token counts for the prompt and the answer along with the question and answer.
        
        Parameters:
            query_text (str): The user's query.
            n_results (int): Number of similar chunks to retrieve.
        
        Returns:
            str: The answer generated by Gemini.
        """
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results
        )

        # Extract the matched chunks (assumed to be in the first result set)
        context_chunks = results.get("documents", [[]])[0]
        context = "\n".join(context_chunks)
        prompt = f"Use the following context to answer the question:\n\n{context}\n\nQuestion: {query_text}"

        # Count tokens being sent to LLM
        tokens_sent = self._count_tokens(prompt)

        # Generate response using the Gemini model
        response = self.gemini_model.generate_content(prompt)
        answer_text = response.text
        tokens_received = self._count_tokens(answer_text)

        # Print details
        print("=== Query Details ===")
        print("Question:", query_text)
        print("Tokens Sent to LLM:", tokens_sent)
        print("Tokens Received from LLM:", tokens_received)
        print("Generated Answer:\n", answer_text)

        return answer_text

# Example usage:
if __name__ == "__main__":
    # Create an instance of the responder with a fixed chunk size of 100 words.
    responder = ChromaDBResponder(collection_name="my_collection", chunk_size=100)
    
    # Ingest a Markdown file (adjust the path as needed).
    markdown_file = "177440d5-3b32-4185-8cc8-95500a9dc783.md"
    responder.ingest_file(markdown_file)
    
    # When a user poses a query:
    user_query = "What key corporate risks are described in the document?"
    responder.query(user_query, n_results=3)
